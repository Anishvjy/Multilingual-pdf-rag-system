# -*- coding: utf-8 -*-
"""Multilingual_PDF_RAG_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kdwJbRKZoqdZ2CB7JVCFZ43jT3xFtl6B
"""

!pip install pytesseract
!pip install opencv-python
!pip install pdf2image
!pip install pdfminer.six
!pip install langdetect
!pip install nltk
!pip install sentence-transformers
!pip install faiss-cpu
!pip install transformers
!pip install fastapi
!pip install uvicorn

!sudo apt install tesseract-ocr

import nltk
nltk.download('punkt')

from pdfminer.high_level import extract_text

def extract_text_from_pdf(pdf_path):
    text = extract_text('/content/drive/MyDrive/RAG_sample data/Blue_Ocean_Strategy,_Expanded_Edition_How_to_Create_Uncontested-2.pdf')
    return text

import pytesseract
from pdf2image import convert_from_path

def ocr_extract_text(pdf_path, lang_code):
    images = convert_from_path(pdf_path)
    text = ''
    for image in images:
        text += pytesseract.image_to_string(image, lang=lang_code)
    return text

from langdetect import detect

def detect_language(text):
    return detect(text)

text preprocessing

import re

def preprocess_text(text):
    # Remove non-alphanumeric characters
    text = re.sub(r'[^\w\s]', '', text)
    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

"""chunking"""

def chunk_text(text, max_length=500):
    sentences = nltk.sent_tokenize(text)
    chunks = []
    current_chunk = ''
    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= max_length:
            current_chunk += ' ' + sentence
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

def embed_text(chunks):
    embeddings = model.encode(chunks)
    return embeddings

import faiss
import numpy as np

def build_faiss_index(embeddings):
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    return index

def search_index(index, query_embedding, k=5):
    D, I = index.search(np.array([query_embedding]), k)
    return I[0], D[0]

def hybrid_search(query, index, embeddings, chunks, k=5):
    # Semantic search
    query_embedding = model.encode([query])
    indices, distances = search_index(index, query_embedding[0], k)
    semantic_results = [chunks[i] for i in indices]

    # Keyword search (simple implementation)
    keyword_results = [chunk for chunk in chunks if query.lower() in chunk.lower()]

    # Combine results
    combined_results = list(set(semantic_results + keyword_results))
    return combined_results

def decompose_query(query):
    # Placeholder for actual decomposition logic
    return [query]

def rerank_results(query, results):
    # Simple reranking based on similarity scores
    query_embedding = model.encode([query])
    result_embeddings = model.encode(results)
    scores = np.dot(result_embeddings, query_embedding[0])
    ranked_results = [result for _, result in sorted(zip(scores, results), reverse=True)]
    return ranked_results

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained('google/mt5-small')
model_llm = AutoModelForSeq2SeqLM.from_pretrained('google/mt5-small')

def generate_answer(context, question):
    input_text = f"question: {question} context: {context}"
    inputs = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)
    outputs = model_llm.generate(inputs, max_length=150)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

class ChatSession:
    def __init__(self):
        self.history = []

    def add_interaction(self, user_input, response):
        self.history.append({'user_input': user_input, 'response': response})

    def get_history(self):
        return self.history

def add_metadata(chunks, language, doc_id):
    chunk_metadata = []
    for i, chunk in enumerate(chunks):
        metadata = {'text': chunk, 'language': language, 'doc_id': doc_id, 'chunk_id': i}
        chunk_metadata.append(metadata)
    return chunk_metadata

def filter_by_metadata(results, language=None, doc_id=None):
    filtered_results = []
    for result in results:
        if language and result['language'] != language:
            continue
        if doc_id and result['doc_id'] != doc_id:
            continue
        filtered_results.append(result)
    return filtered_results

# Example usage
pdf_path = '/content/drive/MyDrive/RAG_sample data/Extension-of-Ahdoc-Employees.pdf'
text = extract_text_from_pdf(pdf_path)
language = detect_language(text)
chunks = preprocess_and_chunk(text)
embeddings = create_embeddings(chunks)
index = build_index(embeddings)
query = "What is the main topic of the document?"
answer = retrieve_and_answer(query, index, chunks)
print("Answer:", answer)

